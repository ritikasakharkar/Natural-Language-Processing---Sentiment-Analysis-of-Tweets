{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/sentiment-analysis-of-tweets/train.txt')\n",
    "train = np.array(train)\n",
    "test = pd.read_csv('/kaggle/input/sentiment-analysis-of-tweets/test_samples.txt')\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NAIVE BAYES CLASSIFIER (ONLY THIS APPROACH WAS SUBMITTED FOR KAGGLE COMPETITION)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to separate the columns of training and test set\n",
    "def separate_columns(t):\n",
    "    ID = []\n",
    "    sen = []\n",
    "    l = []\n",
    "    r, c = t.shape\n",
    "    \n",
    "    # if it is a training data, separate it into 3 columns\n",
    "    if c == 3:                  \n",
    "        for i in range(0, r):\n",
    "            ID.append(t[i][0])\n",
    "            sen.append(t[i][1])\n",
    "            l.append(t[i][2])\n",
    "        return ID, sen, l\n",
    "    \n",
    "    # if it is a testing data, separate it into 2 columns\n",
    "    elif c == 2:\n",
    "        for i in range(0, r):\n",
    "            ID.append(t[i][0])\n",
    "            l.append(t[i][1])\n",
    "        return ID, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing the data\n",
    "def preprocess(l):\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "    r = len(l)\n",
    "    \n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    lem = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    words = nltk.corpus.words.words()\n",
    "    words = ' '.join(words)\n",
    "    words = words.lower()\n",
    "    words = words.split(' ')\n",
    "    \n",
    "    text = []\n",
    "    for i in range(0, r):\n",
    "        l1 = word_tokenize(l[i])                    # tokenize words\n",
    "        l2 = []\n",
    "        for w in l1:\n",
    "            nop = ''\n",
    "            for char in w:                           #remove punctuations\n",
    "                if char not in punctuations:\n",
    "                    nop += char\n",
    "            l5 = nop.lower()\n",
    "            l4 = lem.lemmatize(l5)                    # lemmatize words\n",
    "            if len(l4) > 2:\n",
    "                if l4 in words:                       # add only meaningful words\n",
    "                    if l4 not in stop_words:          # remove stopwords and get filtered sentences with length greater than 2\n",
    "                         l2.append(l4)\n",
    "        text.append(l2)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for counting sentiment frequencies in train data and store it in a dictionary\n",
    "def count_sentiment(sen):\n",
    "    sentiment = {}\n",
    "    c1 = 0\n",
    "    c2 = 0\n",
    "    c3 = 0\n",
    "    r = len(sen)\n",
    "    for i in range(0,r):\n",
    "        if sen[i]=='positive':\n",
    "            c1+=1\n",
    "        elif sen[i] == 'negative':\n",
    "            c2 += 1\n",
    "        elif sen[i] == 'neutral':\n",
    "            c3 += 1\n",
    "    sentiment.update({'positive':c1, 'negative':c2, 'neutral': c3})\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for building vocabulary of train data\n",
    "def vocabulary(txt):\n",
    "    vocab = []\n",
    "    for i in txt:\n",
    "        for w in i:\n",
    "            if w not in vocab:\n",
    "                vocab.append(w)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting frequency of every word in vocubulary for each class\n",
    "# Then storing the frequencies in a 3 dictionaries for 3 classes\n",
    "def freq_of_words_per_sentiment(sen, txt, vocab):\n",
    "    dict_pos = {}\n",
    "    dict_neu = {}\n",
    "    dict_neg = {}\n",
    "\n",
    "    r = len(sen)\n",
    "\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    l3 = []\n",
    "\n",
    "    for i in range(0,r):\n",
    "        if sen[i] == 'positive':\n",
    "            for w in txt[i]:\n",
    "                l1.append(w)\n",
    "        elif sen[i] == 'negative':\n",
    "            for w in txt[i]:\n",
    "                l2.append(w)\n",
    "        elif sen[i] == 'neutral':\n",
    "            for w in txt[i]:\n",
    "                l3.append(w)\n",
    "\n",
    "    str1 = ' '.join(l1)\n",
    "    str2 = ' '.join(l2)\n",
    "    str3 = ' '.join(l3)\n",
    "\n",
    "    for w in vocab:\n",
    "        a = str1.count(w)\n",
    "        b = str2.count(w)\n",
    "        c = str3.count(w)\n",
    "        dict_pos.update({w:a})\n",
    "        dict_neg.update({w:b})\n",
    "        dict_neu.update({w:c})\n",
    "    return dict_pos, dict_neg, dict_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count total number of words in each sentiment class\n",
    "def no_of_words_per_sentiment(dict_pos, dict_neg, dict_neu):\n",
    "    cpos = 0\n",
    "    cneg = 0\n",
    "    cneu = 0\n",
    "    for i in dict_pos:\n",
    "        cpos += dict_pos[i]\n",
    "    for i in dict_neg:\n",
    "        cneg += dict_neg[i]\n",
    "    for i in dict_neu:\n",
    "        cneu += dict_neu[i]\n",
    "    return cpos, cneg, cneu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate probabitity of words in every sentiment\n",
    "def prob_words(vocab, dict_pos, dict_neg, dict_neu):\n",
    "    prob_dict_pos = {}\n",
    "    prob_dict_neg = {}\n",
    "    prob_dict_neu = {}\n",
    "    \n",
    "    cpos, cneg, cneu = no_of_words_per_sentiment(dict_pos, dict_neg, dict_neu)\n",
    "    \n",
    "    for i in dict_pos:\n",
    "        p = (dict_pos[i] + 2)/(cpos + 2*len(vocab))\n",
    "        prob_dict_pos.update({i:p})\n",
    "    for i in dict_neg:\n",
    "        p = (dict_neg[i] + 2)/(cneg + 2*len(vocab))\n",
    "        prob_dict_neg.update({i:p})\n",
    "    for i in dict_neu:\n",
    "        p = (dict_neu[i] + 2)/(cneu + 2*len(vocab))\n",
    "        prob_dict_neu.update({i:p})\n",
    "    return prob_dict_pos, prob_dict_neg, prob_dict_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the probability of occurence of sentiments\n",
    "def prob_sentiments(sen):\n",
    "    import math\n",
    "    \n",
    "    sentiment = count_sentiment(sen)\n",
    "    r = len(sen)\n",
    "    \n",
    "    prob_pos = math.log((sentiment['positive']/r))\n",
    "    prob_neg = math.log((sentiment['negative']/r))\n",
    "    prob_neu = math.log((sentiment['neutral']/r))\n",
    "    \n",
    "    return prob_pos, prob_neg, prob_neu\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the data and predict the sentiment in for each sentence in the test data\n",
    "def predict(train, test)  :  \n",
    "    \n",
    "    import math\n",
    "    predict = []\n",
    "    \n",
    "    i_d, sen, l_train = separate_columns(train)\n",
    "    ID, l = separate_columns(test)\n",
    "    \n",
    "    txt = preprocess(l)\n",
    "    txt_train = preprocess(l_train)\n",
    "    \n",
    "    vocab = vocabulary(txt_train)\n",
    "    dict_pos, dict_neg, dict_neu = freq_of_words_per_sentiment(sen, txt_train, vocab)\n",
    "\n",
    "    prob_pos, prob_neg, prob_neu = prob_sentiments(sen)\n",
    "    prob_dict_pos, prob_dict_neg, prob_dict_neu = prob_words(vocab, dict_pos, dict_neg, dict_neu)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in txt:\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        neu_score = 0 \n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        neu = 0\n",
    "        for w in i:\n",
    "            if w in vocab:\n",
    "                    pos_score += math.log(prob_dict_pos[w])\n",
    "                    neg_score += math.log(prob_dict_neg[w])\n",
    "                    neu_score += math.log(prob_dict_neu[w])\n",
    "        pos = pos_score + prob_pos\n",
    "        neg = neg_score + prob_neg\n",
    "        neu = neu_score + prob_neu\n",
    "        m = max(pos ,neg ,neu )\n",
    "        if m == pos:\n",
    "            predict.append('positive')\n",
    "        elif m == neg:\n",
    "            predict.append('negative')\n",
    "        elif m == neu:\n",
    "            predict.append('neutral')\n",
    "    return  ID, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID , predict = predict(train, test) # THE OUTPUT WHICH IS PRINTED HERE WERE SUBMITTED AS PREDICTIONS FOR KAGGLE COMPETITION\n",
    "n = len(ID) \n",
    "print('tweet_id,sentiment') \n",
    "for i in range(0,n): \n",
    "    print(ID[i],',',predict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **NEURAL NETWORKS APPROACH - LSTM, CNN AND DEEP NEURAL NETWORKS**\n",
    "   \n",
    "   **(I HAVE IMPLEMENTED IT, BUT THIS CODE WAS NOT SUBMITTED FOR KAGGLE COMPETITION)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_D, y, sentences1 = separate_columns(train)\n",
    "i_d, sentences2 = separate_columns(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "X_test = []\n",
    "for sen in sentences1:\n",
    "    X.append(preprocess_text(sen))\n",
    "for sen in sentences2:\n",
    "    X_test.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "### integer mapping using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "\n",
    "### One hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTMs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(vocab_size, 100, trainable=False)(deep_inputs)\n",
    "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "dense_layer_1 = Dense(3, activation='sigmoid')(LSTM_Layer_1)\n",
    "model_LSTM = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "model_LSTM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_LSTM = model_LSTM.fit(X_train, y, batch_size=128, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = model_LSTM.predict(X_test)\n",
    "for i in p1:\n",
    "    a = max(i)\n",
    "    i = list(i)\n",
    "    if i.index(a) == 0:\n",
    "        print('negative')\n",
    "    elif i.index(a) == 1:\n",
    "        print('neutral')\n",
    "    else: \n",
    "        print('positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = Sequential()\n",
    "\n",
    "embedding_layer1 = Embedding(vocab_size, 100, input_length=maxlen , trainable=False)\n",
    "model_CNN.add(embedding_layer1)\n",
    "\n",
    "model_CNN.add(tf.keras.layers.Conv1D(128, 5, activation='relu'))\n",
    "model_CNN.add(GlobalMaxPooling1D())\n",
    "model_CNN.add(Dense(3, activation='sigmoid'))\n",
    "model_CNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_CNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_CNN = model_CNN.fit(X_train, y, batch_size=128, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = model_CNN.predict(X_test)\n",
    "for i in p2:\n",
    "    a = max(i)\n",
    "    i = list(i)\n",
    "    if i.index(a) == 0:\n",
    "        print('negative')\n",
    "    elif i.index(a) == 1:\n",
    "        print('neutral')\n",
    "    else: \n",
    "        print('positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep = Sequential()\n",
    "embedding_layer2 = Embedding(vocab_size, 100, input_length=maxlen , trainable=False)\n",
    "model_deep.add(embedding_layer2)\n",
    "\n",
    "model_deep.add(Flatten())\n",
    "model_deep.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model_deep.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_deep.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_deep = model_deep.fit(X_train, y, batch_size=128, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = model_deep.predict(X_test)\n",
    "for i in p3:\n",
    "    a = max(i)\n",
    "    i = list(i)\n",
    "    if i.index(a) == 0:\n",
    "        print('negative')\n",
    "    elif i.index(a) == 1:\n",
    "        print('neutral')\n",
    "    else: \n",
    "        print('positive')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
